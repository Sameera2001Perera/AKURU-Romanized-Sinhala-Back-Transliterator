{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transliterator.transliteration import Transliterator\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from the local directory\n",
    "model_directory = \"models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_PATH = \"data/dictionary.txt\"\n",
    "\n",
    "# Initialize Transliterator\n",
    "transliterator = Transliterator(\n",
    "    dictionary_path=DICTIONARY_PATH, tokenizer=tokenizer, model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singlish Input: kly rn ha smny\n",
      "Sinhala Output: ‡∂ö‡∑è‡∂Ω‡∂∫ ‡∂ª‡∂±‡∑ä ‡∑Ñ‡∑è ‡∑É‡∂∏‡∑è‡∂±‡∂∫\n"
     ]
    }
   ],
   "source": [
    "# Transliterate input\n",
    "singlish_sentence = input(\"Enter Singlish Sentence: \").strip()\n",
    "print(\"Singlish Input:\", singlish_sentence)\n",
    "sinhala_sentence = transliterator.generate_sinhala(singlish_sentence)\n",
    "print(\"Sinhala Output:\", sinhala_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singlish Input: sadun ad ynw\n",
      "Sinhala Output: ‡∑É‡∂Ø‡∑î‡∂±‡∑ä ‡∂Ö‡∂Ø ‡∂∫‡∂±‡∑Ä‡∑è\n"
     ]
    }
   ],
   "source": [
    "# Transliterate input\n",
    "singlish_sentence = input(\"Enter Singlish Sentence: \").strip()\n",
    "print(\"Singlish Input:\", singlish_sentence)\n",
    "sinhala_sentence = transliterator.generate_sinhala(singlish_sentence)\n",
    "print(\"Sinhala Output:\", sinhala_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singlish Input: hkoj\n",
      "Sinhala Output: ['‡∑Ñ‡∑ä‡∂ö‡∑ú‡∂¢‡∑ä']\n"
     ]
    }
   ],
   "source": [
    "# Transliterate input\n",
    "singlish_sentence = input(\"Enter Singlish Sentence: \").strip()\n",
    "print(\"Singlish Input:\", singlish_sentence)\n",
    "sinhala_sentence = transliterator.generate_sinhala(singlish_sentence)\n",
    "print(\"Sinhala Output:\", sinhala_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romanized Input: awankawama mata eya mathaka ethi akaraya eyayi namuth eya wikarayaki mama obata ashwaya kerehi wedi elmak nodakwana namuth eya mage wilasithawa nowe\n",
      "Expected Sinhala: ‡∂Ö‡∑Ä‡∂Ç‡∂ö‡∑Ä‡∂∏ ‡∂∏‡∂ß ‡∂ë‡∂∫ ‡∂∏‡∂≠‡∂ö ‡∂á‡∂≠‡∑í ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂ë‡∂∫‡∂∫‡∑í ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂ë‡∂∫ ‡∑Ä‡∑í‡∂ö‡∑è‡∂ª‡∂∫‡∂ö‡∑í ‡∂∏‡∂∏ ‡∂î‡∂∂‡∂ß ‡∂Ö‡∑Å‡∑ä‡∑Ä‡∂∫‡∑è ‡∂ö‡∑ô‡∂ª‡∑ô‡∑Ñ‡∑í ‡∑Ä‡∑ê‡∂©‡∑í ‡∂á‡∂Ω‡∑ä‡∂∏‡∂ö‡∑ä ‡∂±‡∑ú‡∂Ø‡∂ö‡∑ä‡∑Ä‡∂± ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂ë‡∂∫ ‡∂∏‡∂ú‡∑ö ‡∑Ä‡∑í‡∂Ω‡∑è‡∑É‡∑í‡∂≠‡∑è‡∑Ä ‡∂±‡∑ú‡∑Ä‡∑ö\n",
      "Generated Sinhala: ‡∂Ö‡∑Ä‡∂Ç‡∂ö‡∑Ä‡∂∏ ‡∂∏‡∂ß ‡∂ë‡∂∫ ‡∂∏‡∂≠‡∂ö ‡∂á‡∂≠‡∑í ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂ë‡∂∫‡∂∫‡∑í ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂ë‡∂∫ ‡∑Ä‡∑í‡∂ö‡∑è‡∂ª‡∂∫‡∂ö‡∑í ‡∂∏‡∂∏ ‡∂î‡∂∂‡∂ß ‡∂Ö‡∑Å‡∑ä‡∑Ä‡∂∫‡∑è ‡∂ö‡∑ô‡∂ª‡∑ô‡∑Ñ‡∑í ‡∑Ä‡∑ê‡∂©‡∑í ‡∂á‡∂Ω‡∑ä‡∂∏‡∂ö‡∑ä ‡∂±‡∑ú‡∂Ø‡∂ö‡∑ä‡∑Ä‡∂± ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂ë‡∂∫ ‡∂∏‡∂ú‡∑ö ‡∑Ä‡∑í‡∂Ω‡∑è‡∑É‡∑í‡∂≠‡∑è‡∑Ä ‡∂±‡∑ú‡∑Ä‡∑ö\n",
      "\n",
      "Romanized Input: oba mage aneka yeyi adahas karanne kese ho oba ema wilasithawata andinne mandeyi mama asami\n",
      "Expected Sinhala: ‡∂î‡∂∂ ‡∂∏‡∂ú‡∑ö ‡∂Ö‡∂±‡∑ô‡∂ö‡∑è ‡∂∫‡∑ê‡∂∫‡∑í ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂ö‡∑ô‡∑É‡∑ö ‡∑Ñ‡∑ù ‡∂î‡∂∂ ‡∂ë‡∂∏ ‡∑Ä‡∑í‡∂Ω‡∑è‡∑É‡∑í‡∂≠‡∑è‡∑Ä‡∂ß ‡∂Ö‡∂≥‡∑í‡∂±‡∑ä‡∂±‡∑ö ‡∂∏‡∂±‡∑ä‡∂Ø‡∑ê‡∂∫‡∑í ‡∂∏‡∂∏ ‡∂Ö‡∑É‡∂∏‡∑í\n",
      "Generated Sinhala: ‡∂î‡∂∂ ‡∂∏‡∂ú‡∑ö ‡∂Ö‡∂±‡∑ô‡∂ö ‡∂∫‡∑ê‡∂∫‡∑í ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂ö‡∑ô‡∑É‡∑ö ‡∑Ñ‡∑ù  ‡∂î‡∂∂ ‡∂ë‡∂∏ ‡∑Ä‡∑í‡∂Ω‡∑è‡∑É‡∑í‡∂≠‡∑è‡∑Ä‡∂ß ‡∂Ö‡∂≥‡∑í‡∂±‡∑ä‡∂±‡∑ö ‡∂∏‡∂±‡∑ä‡∂Ø‡∑ê‡∂∫‡∑í ‡∂∏‡∂∏ ‡∂Ö‡∑É‡∂∏‡∑í\n",
      "\n",
      "Romanized Input: mama kiwa yuthuyi oba wedipura penenne obe mahalu athmayayi oba adahas karanne mage aneka bawayi\n",
      "Expected Sinhala: ‡∂∏‡∂∏ ‡∂ö‡∑í‡∑Ä ‡∂∫‡∑î‡∂≠‡∑î‡∂∫‡∑í ‡∂î‡∂∂ ‡∑Ä‡∑ê‡∂©‡∑í‡∂¥‡∑î‡∂ª ‡∂¥‡∑ô‡∂±‡∑ô‡∂±‡∑ä‡∂±‡∑ö ‡∂î‡∂∂‡∑ö ‡∂∏‡∑Ñ‡∂Ω‡∑î ‡∂Ü‡∂≠‡∑ä‡∂∏‡∂∫‡∂∫‡∑í ‡∂î‡∂∂ ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂∏‡∂ú‡∑ö ‡∂Ö‡∂±‡∑ô‡∂ö‡∑è ‡∂∂‡∑Ä‡∂∫‡∑í\n",
      "Generated Sinhala: ‡∂∏‡∂∏ ‡∂ö‡∑í‡∑Ä ‡∂∫‡∑î‡∂≠‡∑î‡∂∫‡∑í ‡∂î‡∂∂ ‡∑Ä‡∑ê‡∂©‡∑í‡∂¥‡∑î‡∂ª ‡∂¥‡∑ô‡∂±‡∑ô‡∂±‡∑ä‡∂±‡∑ö ‡∂î‡∂∂‡∑ö ‡∂∏‡∑Ñ‡∂Ω‡∑î ‡∂Ü‡∂≠‡∑ä‡∂∏‡∂∫‡∂∫‡∑í ‡∂î‡∂∂ ‡∂Ö‡∂Ø‡∑Ñ‡∑É‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö ‡∂∏‡∂ú‡∑ö ‡∂Ö‡∂±‡∑ô‡∂ö‡∑è ‡∂∂‡∑Ä‡∂∫‡∑í\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:40\u001b[0m\n",
      "File \u001b[1;32me:\\4th Year\\FYP\\IMPLEMENTATION\\transliterator\\transliteration.py:150\u001b[0m, in \u001b[0;36mTransliterator.generate_sinhala\u001b[1;34m(self, singlish_sentence)\u001b[0m\n\u001b[0;32m    147\u001b[0m numbered_sentences \u001b[38;5;241m=\u001b[39m numbering_masks_sentences(sentences)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# print(f\"Numbered sentences: {numbered_sentences}\")\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m filled_sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransliterate(sentences[i], candidates[i])\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences))\n\u001b[0;32m    153\u001b[0m ]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# print(f\"Filled sentences: {filled_sentences}\")\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Find the words for each mask\u001b[39;00m\n\u001b[0;32m    157\u001b[0m mask_words \u001b[38;5;241m=\u001b[39m find_mask_words(numbered_sentences, filled_sentences)\n",
      "File \u001b[1;32me:\\4th Year\\FYP\\IMPLEMENTATION\\transliterator\\transliteration.py:151\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    147\u001b[0m numbered_sentences \u001b[38;5;241m=\u001b[39m numbering_masks_sentences(sentences)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# print(f\"Numbered sentences: {numbered_sentences}\")\u001b[39;00m\n\u001b[0;32m    150\u001b[0m filled_sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransliterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences))\n\u001b[0;32m    153\u001b[0m ]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# print(f\"Filled sentences: {filled_sentences}\")\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Find the words for each mask\u001b[39;00m\n\u001b[0;32m    157\u001b[0m mask_words \u001b[38;5;241m=\u001b[39m find_mask_words(numbered_sentences, filled_sentences)\n",
      "File \u001b[1;32me:\\4th Year\\FYP\\IMPLEMENTATION\\transliterator\\transliteration.py:92\u001b[0m, in \u001b[0;36mTransliterator.transliterate\u001b[1;34m(self, masked_sentence, candidates)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# generate sentences with one blanks including possible candidae words for the blank\u001b[39;00m\n\u001b[0;32m     89\u001b[0m one_blank_sentences \u001b[38;5;241m=\u001b[39m generate_sentences_with_one_blank(\n\u001b[0;32m     90\u001b[0m     word_combinations, mask_indexes, masked_sentence\n\u001b[0;32m     91\u001b[0m )\n\u001b[1;32m---> 92\u001b[0m word_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_probability_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_blank_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m full_sentences \u001b[38;5;241m=\u001b[39m generate_sentences_with_all_combinations(\n\u001b[0;32m     96\u001b[0m     masked_sentence, candidates\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Find the sentence with the highest product\u001b[39;00m\n",
      "File \u001b[1;32me:\\4th Year\\FYP\\IMPLEMENTATION\\transliterator\\transliteration.py:70\u001b[0m, in \u001b[0;36mTransliterator.generate_probability_dict\u001b[1;34m(self, one_blank_sentences, tokenizer)\u001b[0m\n\u001b[0;32m     68\u001b[0m word_probabilities \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m one_blank_sentence, candidate \u001b[38;5;129;01min\u001b[39;00m one_blank_sentences\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 70\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_blank_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(probs)):\n\u001b[0;32m     73\u001b[0m         word \u001b[38;5;241m=\u001b[39m probs[i][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\4th Year\\FYP\\IMPLEMENTATION\\transliterator\\model.py:16\u001b[0m, in \u001b[0;36mMaskedLMModel.generate_probs\u001b[1;34m(self, sentence_with_blank, candidates)\u001b[0m\n\u001b[0;32m     12\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)[\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     17\u001b[0m mask_token_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, mask_token_index, :]\n\u001b[0;32m     18\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(candidates)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1464\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1462\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1479\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def classify_line(line):\n",
    "    \"\"\"Classify the line as Romanized, Sinhala, or blank.\"\"\"\n",
    "    line = line.strip()\n",
    "\n",
    "    # Check if the line is blank\n",
    "    if not line:\n",
    "        return \"Blank\"\n",
    "\n",
    "    # Check if the line contains Sinhala characters (Unicode range 0D80‚Äì0DFF)\n",
    "    if any('\\u0D80' <= char <= '\\u0DFF' for char in line):\n",
    "        return \"Sinhala\"\n",
    "\n",
    "    # If it's not blank and doesn't contain Sinhala characters, it's Romanized\n",
    "    return \"Romanized\"\n",
    "\n",
    "# Open the input file and read the content\n",
    "with open(\"C:/Users/ASUS/Downloads/Sinhala Test set 1 (4).txt\", 'r', encoding='utf-8') as file:\n",
    "    # Read the lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "\n",
    "# Variables to hold Romanized and Sinhala pairs\n",
    "romanized_sentence = None\n",
    "sinhala_sentence = None\n",
    "\n",
    "# Loop through the lines and classify them\n",
    "for i, line in enumerate(lines, start=1):\n",
    "    classification = classify_line(line)\n",
    "\n",
    "    if classification == \"Romanized\":\n",
    "        romanized_sentence = line.strip()  # Store the Romanized line\n",
    "    elif classification == \"Sinhala\":\n",
    "        sinhala_sentence = line.strip()  # Store the Sinhala line\n",
    "\n",
    "    # If both Romanized and Sinhala sentences are identified\n",
    "    if romanized_sentence and sinhala_sentence:\n",
    "        # Count the number of words in the Romanized sentence\n",
    "        word_count = len(romanized_sentence.split())\n",
    "\n",
    "        output = transliterator.generate_sinhala(re.sub(r'\\s+', ' ', romanized_sentence).strip())\n",
    "\n",
    "\n",
    "        print(f\"Romanized Input: {romanized_sentence}\")\n",
    "        print(f\"Expected Sinhala: {sinhala_sentence}\")\n",
    "        print(f\"Generated Sinhala: {output}\\n\")\n",
    "\n",
    "\n",
    "        # Reset the variables after processing the pair\n",
    "        romanized_sentence = None\n",
    "        sinhala_sentence = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinhala_words =  ['‡∂Ö‡∂ö‡∑ä‚Äç‡∂ª‡∑í‡∂∫']\n",
    "valid_words = [word for word in sinhala_words if word in tokenizer.vocab]\n",
    "valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model from the local directory\n",
    "model_directory = \"models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     4,  5753, 22646,     3]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = '[MASK] ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø'\n",
    "input = tokenizer.encode(aa, return_tensors=\"pt\")\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     4,  5753, 22646,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "            aa, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 81\n"
     ]
    }
   ],
   "source": [
    "# Example long text (more than 512 tokens)\n",
    "long_text = \"‡∂î‡∂∂ ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø\" * 20  # Repeated to exceed 512 tokens\n",
    "\n",
    "# Tokenizing without truncation\n",
    "tokens = tokenizer(long_text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens[\"input_ids\"][0]))  # Prints number of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  5773,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "            3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    2,  5773,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,\n",
       "         3809,  3746,  5753, 22646,  3809,  3746,  5753, 22646,  3809,  3746,\n",
       "         5753,     3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing with truncation enabled\n",
    "tokens = tokenizer(long_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens[\"input_ids\"][0]))  # Now within BERT's limit\n",
    "tokens[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([  101,  2023,  2003,  2019,  2742,  6251,  2000, 26268,  1012,   102])\n",
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n",
    "\n",
    "# Sample input text\n",
    "text = \"This is an example sentence to classify.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "print(len(inputs[\"input_ids\"][0]))  # Number of tokens\n",
    "print(inputs[\"input_ids\"][0])\n",
    "\n",
    "\n",
    "# Run model inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[  101,  2460,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1037,  3621,  2936,  6251,  2005,  5604, 11687,\n",
      "          4667,  1012,   102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2182,  2003,  2019,  2130,  2936,  6251,  2000,  4638,  2129,\n",
      "          1996, 19204, 17629, 12033, 11687,  4667,  1998, 19817,  4609,   102]])\n",
      "\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example texts of different lengths\n",
    "texts = [\n",
    "    \"Short sentence.\",\n",
    "    \"This is a slightly longer sentence for testing padding.\",\n",
    "    \"Here is an even longer sentence to check how the tokenizer applies padding and truncation when necessary.\"\n",
    "]\n",
    "\n",
    "# Tokenizing with padding enabled\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "# Print tokenized outputs\n",
    "print(\"Input IDs:\\n\", inputs[\"input_ids\"])\n",
    "print(\"\\nAttention Mask:\\n\", inputs[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "Input IDs:\n",
      " tensor([[  101,  2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,\n",
      "          2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,\n",
      "          3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,\n",
      "          3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324,\n",
      "         19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,\n",
      "          3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005,\n",
      "         14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,\n",
      "          2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,\n",
      "          2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,\n",
      "          6251,  2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,\n",
      "          3039,  2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,\n",
      "          2146,  6251,  2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,\n",
      "          3091,  3039,  2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,\n",
      "          2200,  2146,  6251,  2008,  3397,  3674,  2616,  1998, 23651,  1996,\n",
      "          4555,  3091,  3039,  2005, 14324, 19204,  3989,  1012,  2023,  2003,\n",
      "          1037,  2200,  2146,  6251,  2008,  3397,  3674,  2616,  1998, 23651,\n",
      "          1996,  4555,  3091,  3039,  2005, 14324, 19204,  3989,  1012,  2023,\n",
      "          2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,  2616,  1998,\n",
      "         23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,  3989,  1012,\n",
      "          2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,  2616,\n",
      "          1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,  3989,\n",
      "          1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,\n",
      "          2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,\n",
      "          3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,\n",
      "          3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324,\n",
      "         19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,\n",
      "          3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005,\n",
      "         14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,\n",
      "          2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,\n",
      "          2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,\n",
      "          6251,  2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,\n",
      "          3039,  2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,\n",
      "          2146,  6251,  2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,\n",
      "          3091,  3039,  2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,\n",
      "          2200,  2146,  6251,  2008,  3397,  3674,  2616,  1998, 23651,  1996,\n",
      "          4555,  3091,  3039,  2005, 14324, 19204,  3989,  1012,  2023,  2003,\n",
      "          1037,  2200,  2146,  6251,  2008,  3397,  3674,  2616,  1998, 23651,\n",
      "          1996,  4555,  3091,  3039,  2005, 14324, 19204,  3989,  1012,  2023,\n",
      "          2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,  2616,  1998,\n",
      "         23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,  3989,  1012,\n",
      "          2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,  2616,\n",
      "          1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,  3989,\n",
      "          1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,  3674,\n",
      "          2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324, 19204,\n",
      "          3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,  3397,\n",
      "          3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005, 14324,\n",
      "         19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,  2008,\n",
      "          3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,  2005,\n",
      "         14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,  6251,\n",
      "          2008,  3397,  3674,  2616,  1998, 23651,  1996,  4555,  3091,  3039,\n",
      "          2005, 14324, 19204,  3989,  1012,  2023,  2003,  1037,  2200,  2146,\n",
      "          6251,   102]])\n",
      "\n",
      "Decoded Tokens:\n",
      " ['[CLS]', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', 'that', 'contains', 'multiple', 'words', 'and', 'exceeds', 'the', 'maximum', 'length', 'allowed', 'for', 'bert', 'token', '##ization', '.', 'this', 'is', 'a', 'very', 'long', 'sentence', '[SEP]']\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# A long input text\n",
    "long_text = \"This is a very long sentence that contains multiple words and exceeds the maximum length allowed for BERT tokenization.\"*100\n",
    "\n",
    "# Tokenizing with truncation enabled and a small max_length\n",
    "inputs = tokenizer(long_text, return_tensors=\"pt\", truncation=True)\n",
    "print(len(inputs[\"input_ids\"][0]))  # Number of tokens\n",
    "\n",
    "# Print tokenized outputs\n",
    "print(\"Input IDs:\\n\", inputs[\"input_ids\"])\n",
    "print(\"\\nDecoded Tokens:\\n\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(len(inputs[\"input_ids\"][0]))  # Number of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 8\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create a very long input text (more than 512 words)\n",
    "long_text = \"This is a long sentence. \"   # Repeats to exceed 512 tokens\n",
    "\n",
    "# Tokenizing without truncation\n",
    "inputs = tokenizer(long_text, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "# Print total token count\n",
    "token_count = len(inputs[\"input_ids\"][0])\n",
    "print(f\"Total Tokens: {token_count}\")\n",
    "\n",
    "# Checking if it exceeds 512 tokens\n",
    "if token_count > 512:\n",
    "    print(\"‚ö†Ô∏è Warning: The input exceeds 512 tokens and will cause an error in BERT!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Attempt to pass the long input into BERT\n",
    "outputs = model(**inputs)  # ‚ùå This will cause an error!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs no pad: tensor([1.1037e-07, 1.1283e-07, 1.4856e-07,  ..., 1.5335e-07, 2.7046e-07,\n",
      "        9.8091e-08])\n",
      "Probs with pad: tensor([1.1037e-07, 1.1283e-07, 1.4856e-07,  ..., 1.5335e-07, 2.7046e-07,\n",
      "        9.8091e-08])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load BERT tokenizer and model for Masked Language Modeling\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Define a sentence with a masked token\n",
    "text = \"This is a [MASK] example.\"\n",
    "\n",
    "# Tokenize without padding\n",
    "inputs_no_pad = tokenizer(text, return_tensors=\"pt\", padding=False, truncation=True)\n",
    "\n",
    "# Tokenize with padding (max_length=10)\n",
    "inputs_with_pad = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=30)\n",
    "\n",
    "# Get the index of the masked token\n",
    "mask_index_no_pad = torch.where(inputs_no_pad[\"input_ids\"] == tokenizer.mask_token_id)[1].item()\n",
    "mask_index_with_pad = torch.where(inputs_with_pad[\"input_ids\"] == tokenizer.mask_token_id)[1].item()\n",
    "\n",
    "# Run model inference\n",
    "with torch.no_grad():\n",
    "    logits_no_pad = model(**inputs_no_pad).logits\n",
    "    logits_with_pad = model(**inputs_with_pad).logits\n",
    "\n",
    "# Get softmax probabilities for the masked token position\n",
    "probs_no_pad = F.softmax(logits_no_pad[0, mask_index_no_pad], dim=-1)\n",
    "probs_with_pad = F.softmax(logits_with_pad[0, mask_index_with_pad], dim=-1)\n",
    "\n",
    "print(\"Probs no pad:\", probs_no_pad)\n",
    "print(\"Probs with pad:\", probs_with_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, tokenizer, sentence, candidates):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    mask_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    mask_word_logits = logits[0, mask_index, :]\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(mask_word_logits, dim=-1)\n",
    "    \n",
    "    candidate_probs = {}\n",
    "    for word in candidates:\n",
    "        word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "        if word_id is not None:\n",
    "            candidate_probs[word] = probabilities[0, word_id].item()\n",
    "        else:\n",
    "            candidate_probs[word] = 0.0  # If the word is not in vocab, assign 0 probability\n",
    "    \n",
    "    return sentence, candidate_probs\n",
    "\n",
    "def process_inputs(input_dict):    \n",
    "    results = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        future_to_sentence = {\n",
    "            executor.submit(get_predictions, model, tokenizer, sentence, candidates): sentence\n",
    "            for sentence, candidates in input_dict.items()\n",
    "        }\n",
    "        \n",
    "        for future in future_to_sentence:\n",
    "            sentence, probs = future.result()\n",
    "            results[sentence] = probs\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "input_dict = {\n",
    "    \"[MASK] will come\": [\"i\", \"car\"], \n",
    "    \"[MASK] can go\": [\"run\", \"we\"]\n",
    "}\n",
    "\n",
    "output = process_inputs(input_dict)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "class MaskedLMModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_probs(self, sentences_with_blank, candidate_dict):\n",
    "        # Tokenize all sentences in batch\n",
    "        inputs = self.tokenizer(\n",
    "            sentences_with_blank, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "        # Identify mask positions in batch\n",
    "        mask_token_indices = (inputs.input_ids == self.tokenizer.mask_token_id).nonzero(\n",
    "            as_tuple=True\n",
    "        )\n",
    "\n",
    "        # Perform forward pass in parallel\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(\n",
    "                **inputs\n",
    "            ).logits  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        word_probabilities = {}\n",
    "        for i, sentence in enumerate(sentences_with_blank):\n",
    "            mask_pos = mask_token_indices[1][\n",
    "                i\n",
    "            ].item()  # Get mask index for this sentence\n",
    "            mask_logits = logits[i, mask_pos, :]  # Extract logits for mask position\n",
    "\n",
    "            candidates = candidate_dict[sentence]\n",
    "            word_ids = self.tokenizer.convert_tokens_to_ids(candidates)\n",
    "            word_probs = F.softmax(mask_logits, dim=-1)[word_ids].tolist()\n",
    "\n",
    "            # Store probabilities for each word\n",
    "            for j, word in enumerate(candidates):\n",
    "                word_probabilities[(sentence, word)] = word_probs[j]\n",
    "\n",
    "        return word_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "tokenizer = BertTokenizer.from_pretrained(\"models\")\n",
    "model = BertForMaskedLM.from_pretrained(\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_model = MaskedLMModel(model, tokenizer)\n",
    "\n",
    "input_dict = {\n",
    "    # \"[MASK] ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂î‡∂∂\", \"‡∂î‡∂∂‡∑è\"],\n",
    "    \"‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂Ö‡∂Ø\", \"‡∂Ü‡∂Ø‡∑ì\",]\n",
    "}\n",
    "\n",
    "sentences = list(input_dict.keys())\n",
    "output = masked_lm_model.generate_probs(sentences, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø', '‡∂Ö‡∂Ø'): 0.039673857390880585,\n",
       " ('‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø', '‡∂Ü‡∂Ø‡∑ì'): 3.4286035770492163e-06}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('[MASK] will come', 'i'): 0.00018806032312568277,\n",
       " ('[MASK] will come', 'car'): 6.636828766204417e-05,\n",
       " ('[MASK] can go', 'run'): 1.5131897271203343e-05,\n",
       " ('[MASK] can go', 'we'): 1.5131897271203343e-05}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_model2 = MaskedLMModel(model2, tokenizer2)\n",
    "\n",
    "input_dict2 = {\n",
    "    \"[MASK] will come\": [\"i\", \"car\"],\n",
    "    \"[MASK] can go\": [\"run\", \"we\"]\n",
    "}\n",
    "\n",
    "\n",
    "sentences2 = list(input_dict2.keys())\n",
    "output2 = masked_lm_model.generate_probs(sentences2, input_dict2)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.636828766204417e-05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[('[MASK] will come', 'car')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability1 of 'car' in '[MASK] will come': 6.636828766204417e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability1 of 'car' in '[MASK] will come': {output2[('[MASK] will come', 'car')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('[MASK] will come', 'i'): 0.00018806042498908937,\n",
       " ('[MASK] will come', 'car'): 6.63683531456627e-05}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict2 = {\n",
    "    \"[MASK] will come\": [\"i\", \"car\"],\n",
    "    # \"[MASK] can go\": [\"run\", \"we\"]\n",
    "}\n",
    "\n",
    "\n",
    "sentences2 = list(input_dict2.keys())\n",
    "output2 = masked_lm_model.generate_probs(sentences2, input_dict2)\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability2 of 'car' in '[MASK] will come': 6.63683531456627e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability2 of 'car' in '[MASK] will come': {output2[('[MASK] will come', 'car')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('[MASK] will come', 'i'): 0.00018806032312568277,\n",
       " ('[MASK] will come', 'car'): 6.636828766204417e-05,\n",
       " ('[MASK] can go', 'run'): 1.5131897271203343e-05,\n",
       " ('[MASK] can go', 'we'): 1.5131897271203343e-05,\n",
       " ('[MASK] eat cat', 'it'): 6.345955625874922e-06,\n",
       " ('[MASK] eat cat', 'rat'): 6.345955625874922e-06}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict3 = {\n",
    "    \"[MASK] will come\": [\"i\", \"car\"],\n",
    "    \"[MASK] can go\": [\"run\", \"we\"],\n",
    "    \"[MASK] eat cat\": [\"it\", \"rat\"]\n",
    "}\n",
    "\n",
    "\n",
    "sentences3 = list(input_dict3.keys())\n",
    "output3 = masked_lm_model.generate_probs(sentences3, input_dict3)\n",
    "output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "only single target (not tuple) can be annotated (3696649686.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ('[MASK] will come', 'car'): 6.636828766204417e-05,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m only single target (not tuple) can be annotated\n"
     ]
    }
   ],
   "source": [
    " ('[MASK] will come', 'car'): 6.636828766204417e-05,\n",
    " ('[MASK] will come', 'car'): 6.636828766204417e-05,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "class MaskedLMModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_probs(self, sentences_with_blank, candidate_dict):\n",
    "        # Consistent tokenization\n",
    "        inputs = self.tokenizer(\n",
    "            sentences_with_blank, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            max_length=10  # Adjust based on your expected sentence length\n",
    "        )\n",
    "\n",
    "        mask_token_indices = (inputs.input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "\n",
    "        word_probabilities = {}\n",
    "        for i, sentence in enumerate(sentences_with_blank):\n",
    "            mask_pos = mask_token_indices[1][i].item()\n",
    "            mask_logits = logits[i, mask_pos, :]\n",
    "\n",
    "            candidates = candidate_dict[sentence]\n",
    "            word_ids = self.tokenizer.convert_tokens_to_ids(candidates)\n",
    "            word_probs = F.softmax(mask_logits, dim=-1)[word_ids].tolist()\n",
    "\n",
    "            for j, word in enumerate(candidates):\n",
    "                word_probabilities[(sentence, word)] = word_probs[j]\n",
    "\n",
    "        return word_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\") \n",
    "model.eval()\n",
    "\n",
    "masked_lm_model = MaskedLMModel(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict1 = { \n",
    "    \"[MASK] will come\": [\"i\", \"car\"],\n",
    "    \"[MASK] can go\": [\"run\", \"we\"]\n",
    "}\n",
    "\n",
    "input_dict2 = {\n",
    "    \"[MASK] will come\": [\"i\", \"car\"],\n",
    "}\n",
    "\n",
    "sentences1 = list(input_dict1.keys()) \n",
    "sentences2 = list(input_dict2.keys()) \n",
    "\n",
    "output1 = masked_lm_model.generate_probs(sentences1, input_dict1) \n",
    "output2 = masked_lm_model.generate_probs(sentences2, input_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability1 of 'car' in '[MASK] will come': 8.749316293688025e-06\n",
      "Probability2 of 'car' in '[MASK] will come': 8.749316293688025e-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability1 of 'car' in '[MASK] will come': {output1[('[MASK] will come', 'car')]}\")\n",
    "print(f\"Probability2 of 'car' in '[MASK] will come': {output2[('[MASK] will come', 'car')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : {'input_ids': tensor([[    2,     4,  5753, 22646,     3],\n",
      "        [    2,  5773,     4, 22646,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n",
      "Input : {'input_ids': tensor([[    2,  5773,     4, 22646,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "Probability1 of 'car' in '[MASK] will come': 0.03967390954494476\n",
      "Probability2 of 'car' in '[MASK] will come': 0.039673857390880585\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "class MaskedLMModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_probs(self, sentences_with_blank, candidate_dict):\n",
    "        # Tokenize all sentences in batch\n",
    "        inputs = self.tokenizer(\n",
    "            sentences_with_blank, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "        print(f\"Input : {inputs}\")\n",
    "\n",
    "        # Identify mask positions in batch\n",
    "        mask_token_indices = (inputs.input_ids == self.tokenizer.mask_token_id).nonzero(\n",
    "            as_tuple=True\n",
    "        )\n",
    "\n",
    "        # Perform forward pass in parallel\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(\n",
    "                **inputs\n",
    "            ).logits  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        word_probabilities = {}\n",
    "        for i, sentence in enumerate(sentences_with_blank):\n",
    "            mask_pos = mask_token_indices[1][\n",
    "                i\n",
    "            ].item()  # Get mask index for this sentence\n",
    "            mask_logits = logits[i, mask_pos, :]  # Extract logits for mask position\n",
    "\n",
    "            candidates = candidate_dict[sentence]\n",
    "            word_ids = self.tokenizer.convert_tokens_to_ids(candidates)\n",
    "            word_probs = F.softmax(mask_logits, dim=-1)[word_ids].tolist()\n",
    "\n",
    "            # Store probabilities for each word\n",
    "            for j, word in enumerate(candidates):\n",
    "                word_probabilities[(sentence, word)] = word_probs[j]\n",
    "\n",
    "        return word_probabilities\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"models\") \n",
    "model = BertForMaskedLM.from_pretrained(\"models\") \n",
    "model.eval()\n",
    "\n",
    "masked_lm_model = MaskedLMModel(model, tokenizer) \n",
    "\n",
    "input_dict1 = {\n",
    "    \"[MASK] ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂î‡∂∂\", \"‡∂î‡∂∂‡∑è\"],\n",
    "    \"‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂Ö‡∂Ø\", \"‡∂Ü‡∂Ø‡∑ì\",]\n",
    "}\n",
    "\n",
    "input_dict2 = {\n",
    "    # \"[MASK] ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂î‡∂∂\", \"‡∂î‡∂∂‡∑è\"],\n",
    "    \"‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂Ö‡∂Ø\", \"‡∂Ü‡∂Ø‡∑ì\",]\n",
    "}\n",
    "\n",
    "sentences1 = list(input_dict1.keys()) \n",
    "sentences2 = list(input_dict2.keys()) \n",
    "\n",
    "output1 = masked_lm_model.generate_probs(sentences1, input_dict1) \n",
    "output2 = masked_lm_model.generate_probs(sentences2, input_dict2)\n",
    "\n",
    "print(f\"Probability1 of 'car' in '[MASK] will come': {output1[('‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø', '‡∂Ö‡∂Ø')]}\")\n",
    "print(f\"Probability2 of 'car' in '[MASK] will come': {output2[('‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø', '‡∂Ö‡∂Ø')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : {'input_ids': tensor([[   2,    4, 5753,  ...,    0,    0,    0],\n",
      "        [   2, 5773,    4,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m sentences1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_dict1\u001b[38;5;241m.\u001b[39mkeys()) \n\u001b[0;32m     75\u001b[0m sentences2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(input_dict2\u001b[38;5;241m.\u001b[39mkeys()) \n\u001b[1;32m---> 77\u001b[0m output1 \u001b[38;5;241m=\u001b[39m \u001b[43mmasked_lm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict1\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     78\u001b[0m output2 \u001b[38;5;241m=\u001b[39m masked_lm_model\u001b[38;5;241m.\u001b[39mgenerate_probs(sentences2, input_dict2)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbability1 of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[MASK] will come\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput1[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m‡∂Ö‡∂Ø\u001b[39m\u001b[38;5;124m'\u001b[39m)]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 37\u001b[0m, in \u001b[0;36mMaskedLMModel.generate_probs\u001b[1;34m(self, sentences_with_blank, candidate_dict)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Perform forward pass in parallel\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\n\u001b[0;32m     39\u001b[0m     )\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m word_probabilities \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences_with_blank):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1464\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1462\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1479\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1078\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1076\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1078\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:217\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    216\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 217\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    218\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    219\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "class MaskedLMModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_probs(self, sentences_with_blank, candidate_dict):\n",
    "        # Tokenize all sentences in batch\n",
    "        # inputs = self.tokenizer(\n",
    "        #     sentences_with_blank, \n",
    "        #     return_tensors=\"pt\", \n",
    "        #     padding=True, \n",
    "        #     truncation=True\n",
    "        # )\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            sentences_with_blank, \n",
    "            return_tensors=\"pt\", \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            max_length=10  # set max_length explicitly to ensure consistency\n",
    "        )\n",
    "\n",
    "        print(f\"Input : {inputs}\")\n",
    "\n",
    "\n",
    "        # Identify mask positions in batch\n",
    "        mask_token_indices = (inputs.input_ids == self.tokenizer.mask_token_id).nonzero(\n",
    "            as_tuple=True\n",
    "        )\n",
    "\n",
    "        # Perform forward pass in parallel\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(\n",
    "                **inputs\n",
    "            ).logits  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        word_probabilities = {}\n",
    "        for i, sentence in enumerate(sentences_with_blank):\n",
    "            mask_pos = mask_token_indices[1][\n",
    "                i\n",
    "            ].item()  # Get mask index for this sentence\n",
    "            mask_logits = logits[i, mask_pos, :]  # Extract logits for mask position\n",
    "\n",
    "            candidates = candidate_dict[sentence]\n",
    "            word_ids = self.tokenizer.convert_tokens_to_ids(candidates)\n",
    "            word_probs = F.softmax(mask_logits, dim=-1)[word_ids].tolist()\n",
    "\n",
    "            # Store probabilities for each word\n",
    "            for j, word in enumerate(candidates):\n",
    "                word_probabilities[(sentence, word)] = word_probs[j]\n",
    "\n",
    "        return word_probabilities\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"models\") \n",
    "model = BertForMaskedLM.from_pretrained(\"models\") \n",
    "model.eval()\n",
    "\n",
    "masked_lm_model = MaskedLMModel(model, tokenizer) \n",
    "\n",
    "input_dict1 = {\n",
    "    \"[MASK] ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂î‡∂∂\", \"‡∂î‡∂∂‡∑è\"],\n",
    "    \"‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂Ö‡∂Ø\", \"‡∂Ü‡∂Ø‡∑ì\",]\n",
    "}\n",
    "\n",
    "input_dict2 = {\n",
    "    # \"[MASK] ‡∂Ö‡∂Ø ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂î‡∂∂\", \"‡∂î‡∂∂‡∑è\"],\n",
    "    \"‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø\": [\"‡∂Ö‡∂Ø\", \"‡∂Ü‡∂Ø‡∑ì\",]\n",
    "}\n",
    "\n",
    "sentences1 = list(input_dict1.keys()) \n",
    "sentences2 = list(input_dict2.keys()) \n",
    "\n",
    "output1 = masked_lm_model.generate_probs(sentences1, input_dict1) \n",
    "output2 = masked_lm_model.generate_probs(sentences2, input_dict2)\n",
    "\n",
    "print(f\"Probability1 of 'car' in '[MASK] will come': {output1[('‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø', '‡∂Ö‡∂Ø')]}\")\n",
    "print(f\"Probability2 of 'car' in '[MASK] will come': {output2[('‡∂î‡∂∂ [MASK] ‡∂ë‡∂±‡∑Ä‡∂Ø', '‡∂Ö‡∂Ø')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\") \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 16 is okay!\n",
      "Batch size 32 is okay!\n",
      "Batch size 64 is okay!\n",
      "Batch size 128 is okay!\n",
      "Batch size 256 is okay!\n",
      "Batch size 512 is okay!\n",
      "Batch size 1024 is okay!\n",
      "Batch size 2048 is okay!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK] will come\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m batch_size, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is okay!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     batch_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# double it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1464\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1462\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1479\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:466\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16  # start from a reasonable guess\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        inputs = tokenizer([\"[MASK] will come\"] * batch_size, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)\n",
    "        logits = model(**inputs).logits\n",
    "        print(f\"Batch size {batch_size} is okay!\")\n",
    "        batch_size *= 2  # double it\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Reached memory limit at batch size {batch_size}: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 16 OK\n"
     ]
    }
   ],
   "source": [
    "# Example empirical test to find a suitable batch size\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "\n",
    "while batch_size > 0:\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            [\"[MASK] will come\"] * batch_size,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        logits = model(**inputs).logits\n",
    "        print(f\"Batch size {batch_size} OK\")\n",
    "        break\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Batch size {batch_size} failed: {e}\")\n",
    "        batch_size //= 2  # reduce batch size by half\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "class MaskedLMModel:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_probs(self, sentences_with_blank, candidate_dict):\n",
    "        # Consistent tokenization\n",
    "        inputs = self.tokenizer(\n",
    "            sentences_with_blank, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        print(f\"types: {type(inputs)}\")\n",
    "        print(f\"length: {len(inputs['input_ids'])}\")\n",
    "        print(f\"token length 1: {len(inputs['input_ids'][0])}\")\n",
    "        print(f\"token length 2: {len(inputs['input_ids'][1])}\")\n",
    "        print(f\"Input : {inputs}\")\n",
    "\n",
    "        mask_token_indices = (inputs.input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "        print(f\"mask_token_indices: {mask_token_indices}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "\n",
    "        word_probabilities = {}\n",
    "        for i, sentence in enumerate(sentences_with_blank):\n",
    "            mask_pos = mask_token_indices[1][i].item()\n",
    "            mask_logits = logits[i, mask_pos, :]\n",
    "\n",
    "            candidates = candidate_dict[sentence]\n",
    "            word_ids = self.tokenizer.convert_tokens_to_ids(candidates)\n",
    "            word_probs = F.softmax(mask_logits, dim=-1)[word_ids].tolist()\n",
    "\n",
    "            for j, word in enumerate(candidates):\n",
    "                word_probabilities[(sentence, word)] = word_probs[j]\n",
    "\n",
    "        return word_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\") \n",
    "model.eval()\n",
    "\n",
    "masked_lm_model = MaskedLMModel(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = (\"come \"*508) + \"tomorrow [MASK]\"\n",
    "inp2 = (\"run \"*505) + \"today [MASK]\"\n",
    "inp3 = (\"walk \"*505) + \"today [MASK]\"\n",
    "inp4 = (\"jump \"*505) + \"today [MASK]\"\n",
    "inp5 = (\"sit \"*505) + \"today [MASK]\"\n",
    "inp6 = (\"stand \"*505) + \"today [MASK]\"\n",
    "inp7 = (\"sleep \"*505) + \"today [MASK]\"\n",
    "inp8 = (\"eat \"*505) + \"today [MASK]\"\n",
    "inp9 = (\"drink \"*505) + \"today [MASK]\"\n",
    "inp10 = (\"play \"*505) + \"today [MASK]\"\n",
    "inp11 = (\"study \"*505) + \"today [MASK]\"\n",
    "inp12 = (\"work \"*505) + \"today [MASK]\"\n",
    "inp13 = (\"write \"*505) + \"today [MASK]\"\n",
    "inp14 = (\"read \"*505) + \"today [MASK]\"\n",
    "inp15 = (\"sing \"*505) + \"today [MASK]\"\n",
    "inp16 = (\"dance \"*505) + \"today [MASK]\"\n",
    "inp17 = (\"cook \"*505) + \"today [MASK]\"\n",
    "inp18 = (\"clean \"*505) + \"today [MASK]\"\n",
    "inp19 = (\"wash \"*505) + \"today [MASK]\"\n",
    "inp20 = (\"paint \"*505) + \"today [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    inp: [\"i\", \"you\"],\n",
    "    inp2: [\"we\", \"they\"],\n",
    "    inp3: [\"he\", \"she\"],\n",
    "    inp4: [\"it\", \"they\"],\n",
    "    inp5: [\"we\", \"they\"],\n",
    "    inp6: [\"he\", \"she\"],\n",
    "    inp7: [\"it\", \"they\"],\n",
    "    inp8: [\"we\", \"they\"],\n",
    "    inp9: [\"he\", \"she\"],\n",
    "    inp10: [\"it\", \"they\"],\n",
    "    inp11: [\"we\", \"they\"],\n",
    "    inp12: [\"he\", \"she\"],\n",
    "    inp13: [\"it\", \"they\"],\n",
    "    inp14: [\"we\", \"they\"],\n",
    "    inp15: [\"he\", \"she\"],\n",
    "    inp16: [\"it\", \"they\"],\n",
    "    inp17: [\"we\", \"they\"],\n",
    "    inp18: [\"he\", \"she\"],\n",
    "    inp19: [\"it\", \"they\"],\n",
    "    inp20: [\"we\", \"they\"],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "length: 20\n",
      "token length 1: 512\n",
      "token length 2: 512\n",
      "Input : {'input_ids': tensor([[ 101, 2272, 2272,  ..., 4826,  103,  102],\n",
      "        [ 101, 2448, 2448,  ...,    0,    0,    0],\n",
      "        [ 101, 3328, 3328,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4550, 4550,  ...,    0,    0,    0],\n",
      "        [ 101, 9378, 9378,  ...,    0,    0,    0],\n",
      "        [ 101, 6773, 6773,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "mask_token_indices: (tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19]), tensor([510, 507, 507, 507, 507, 507, 507, 507, 507, 507, 507, 507, 507, 507,\n",
      "        507, 507, 507, 507, 507, 507]))\n"
     ]
    }
   ],
   "source": [
    "# input_dict = {  \n",
    "#     inp: [\"i\", \"car\"],\n",
    "#     inp2: [\"i\", \"car\"]\n",
    "\n",
    "# } \n",
    "sentences = list(input_dict.keys())   \n",
    "\n",
    "output = masked_lm_model.generate_probs(sentences, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "length: 1\n",
      "token length: 512\n",
      "Input : {'input_ids': tensor([[ 101, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272, 2272,\n",
      "         2272, 2272, 2272, 2272, 2272, 4826,  103,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "mask_token_indices: (tensor([0]), tensor([510]))\n"
     ]
    }
   ],
   "source": [
    "input_dict = {  \n",
    "    inp: [\"i\", \"car\"],\n",
    "    inp2: [\"i\", \"car\"]\n",
    "\n",
    "} \n",
    "sentences = list(input_dict.keys())   \n",
    "\n",
    "output = masked_lm_model.generate_probs(sentences, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
