# Romanized Sinhala (Singlish) to Sinhala Reverse Transliteration Using BERT

![License](https://img.shields.io/badge/License-MIT-blue.svg)  
![Python](https://img.shields.io/badge/Python-3.8%2B-blue)  
![Jupyter Notebook](https://img.shields.io/badge/Notebook-Jupyter-orange)  

## üìå Overview  

This repository contains a **context-aware transliteration system** for converting **Romanized Sinhala (Singlish)** to **native Sinhala script** using a **hybrid approach** that leverages:  

- **Dictionary-Based Mapping** for common words  
- **Rule-Based Techniques** for out-of-vocabulary words  
- **BERT Language Model** for contextual disambiguation  

This implementation is based on the research paper:  
[üìÑ *IndoNLP 2025 Shared Task: Romanized Sinhala to Sinhala Reverse Transliteration Using BERT*](https://aclanthology.org/2025.indonlp-1.16/)

## üèó Features  

‚úÖ Handles **informal Singlish typing patterns** (e.g., vowel omissions)  
‚úÖ Resolves **lexical ambiguity** (e.g., "Adaraya" ‚Üí "‡∂Ü‡∂Ø‡∂ª‡∂∫" or "‡∂Ü‡∂∞‡∑è‡∂ª‡∂∫") using **BERT-based disambiguation**  
‚úÖ **Rule-based transliteration** for unknown words  
‚úÖ Supports **chunking** to optimize long sentence processing  
‚úÖ Achieves **high BLEU scores & low Word/Character Error Rates**  

## üìñ How It Works
### 1Ô∏è‚É£ Preprocessing:
- The input Singlish text is split into words.
- A Singlish-Sinhala dictionary is used to map words if available.
- If not found, a rule-based transliteration method is applied.
### 2Ô∏è‚É£ Masked Sentence Generation:
- Words with multiple possible Sinhala equivalents are replaced with [MASK] tokens.
### 3Ô∏è‚É£ BERT-Based Context Disambiguation:
- A Sinhala BERT model is used to predict the most contextually appropriate word.
- The final sentence is generated by selecting the highest-probability words.
### 4Ô∏è‚É£ Chunking for Long Sentences:
- To optimize efficiency, sentences with many ambiguous words are chunked before processing.

## üìä Evaluation
The evaluation was based on the [validation test sets](https://github.com/IndoNLP-Workshop/IndoNLP-2025-Shared-Task) provided by the INDONLP 2025 shared task organizers.

‚úÖ BLEU Score: 0.9+  
‚úÖ Word Error Rate (WER): Below 0.092
‚úÖ Character Error Rate (CER): Below 0.022

The model efficiently handles lexical ambiguity and informal Singlish typing styles, improving transliteration quality.

## üìù Limitations
‚ö†Ô∏è Processing Speed:  
- Long sentences with many ambiguous words require multiple BERT inferences, increasing runtime.  

‚ö†Ô∏è Rule-Based Component:  
- The rule-based transliteration works for general cases but does not handle all informal typing patterns.  

## üìú Citation
If you use this code in your research, please cite:
```sh
@inproceedings{perera-etal-2025-indonlp,
    title = "{I}ndo{NLP} 2025 Shared Task: {R}omanized {S}inhala to {S}inhala Reverse Transliteration Using {BERT}",
    author = "Perera, Sandun Sameera  and
      Jayakodi, Lahiru Prabhath  and
      Sumanathilaka, Deshan Koshala  and
      Anuradha, Isuri",
    editor = "Weerasinghe, Ruvan  and
      Anuradha, Isuri  and
      Sumanathilaka, Deshan",
    booktitle = "Proceedings of the First Workshop on Natural Language Processing for Indo-Aryan and Dravidian Languages",
    month = jan,
    year = "2025",
    address = "Abu Dhabi",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.indonlp-1.16/",
    pages = "135--140",
    abstract = "The Romanized text has become popu lar with the growth of digital communi cation platforms, largely due to the fa miliarity with English keyboards. In Sri Lanka, Romanized Sinhala, commonly re ferred to as {\textquotedblleft}Singlish{\textquotedblright} is widely used in digi tal communications. This paper introduces a novel context-aware back-transliteration system designed to address the ad-hoc typ ing patterns and lexical ambiguity inher ent in Singlish. The proposed system com bines dictionary-based mapping for Singlish words, a rule-based transliteration for out of-vocabulary words and a BERT-based language model for addressing lexical am biguities. Evaluation results demonstrate the robustness of the proposed approach, achieving high BLEU scores along with low Word Error Rate (WER) and Character Er ror Rate (CER) across test datasets. This study provides an effective solution for Ro manized Sinhala back-transliteration and establishes the foundation for improving NLP tools for similar low-resourced lan guages."
}
